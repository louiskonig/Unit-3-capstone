{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>is_exceptional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
       "      <td>Martha's Vineyard</td>\n",
       "      <td>96</td>\n",
       "      <td>235.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Heitz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>Ripe aromas of fig, blackberry and cassis are ...</td>\n",
       "      <td>Carodorum Selección Especial Reserva</td>\n",
       "      <td>96</td>\n",
       "      <td>110.0</td>\n",
       "      <td>Northern Spain</td>\n",
       "      <td>Toro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tinta de Toro</td>\n",
       "      <td>Bodega Carmen Rodríguez</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country                                        description  \\\n",
       "0      US  This tremendous 100% varietal wine hails from ...   \n",
       "1   Spain  Ripe aromas of fig, blackberry and cassis are ...   \n",
       "\n",
       "                            designation  points  price        province  \\\n",
       "0                     Martha's Vineyard      96  235.0      California   \n",
       "1  Carodorum Selección Especial Reserva      96  110.0  Northern Spain   \n",
       "\n",
       "      region_1 region_2             variety                   winery  \\\n",
       "0  Napa Valley     Napa  Cabernet Sauvignon                    Heitz   \n",
       "1         Toro      NaN       Tinta de Toro  Bodega Carmen Rodríguez   \n",
       "\n",
       "   is_exceptional  \n",
       "0               1  \n",
       "1               1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data\n",
    "df = pd.read_csv('winemag-data_first150k.csv')\n",
    "\n",
    "#Insert column to flag \"Exceptional wines\"\n",
    "df['is_exceptional'] = np.where(df['points']>=90, 1, 0)\n",
    "\n",
    "#set null prices to the mean price\n",
    "df.loc[pd.isnull(df.price),'price']=df.price.mean()\n",
    "\n",
    "#Drop unnamed col\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df.head(2)\n",
    "#country, description, designation, points, price, province, region_1, region_2, variety, winery, is_exceptional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg word function\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Add feaures\n",
    "\n",
    "#text features\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df['word_count'] = df['description'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df['char_count'] = df['description'].str.len() ## this also includes spaces\n",
    "df['avg_word'] = df['description'].apply(lambda x: avg_word(x))\n",
    "df['stopwords'] = df['description'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "df['numerics'] = df['description'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "df['percent'] = df['description'].apply(lambda x: len([x for x in x.split() if x.endswith('%')]))\n",
    "\n",
    "#description processing\n",
    "df['description'] = df['description'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df['description'] = df['description'].str.replace('[^\\w\\s]','')\n",
    "df['description'] = df['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>is_exceptional</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>numerics</th>\n",
       "      <th>percent</th>\n",
       "      <th>country_Albania</th>\n",
       "      <th>...</th>\n",
       "      <th>variety_Xynisteri</th>\n",
       "      <th>variety_Yapincak</th>\n",
       "      <th>variety_Zelen</th>\n",
       "      <th>variety_Zibibbo</th>\n",
       "      <th>variety_Zierfandler</th>\n",
       "      <th>variety_Zierfandler-Rotgipfler</th>\n",
       "      <th>variety_Zinfandel</th>\n",
       "      <th>variety_Zlahtina</th>\n",
       "      <th>variety_Zweigelt</th>\n",
       "      <th>variety_Žilavka</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96</td>\n",
       "      <td>235.0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>355</td>\n",
       "      <td>4.933333</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>318</td>\n",
       "      <td>5.254902</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 2398 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   points  price  is_exceptional  word_count  char_count  avg_word  stopwords  \\\n",
       "0      96  235.0               1          60         355  4.933333         23   \n",
       "1      96  110.0               1          51         318  5.254902         18   \n",
       "\n",
       "   numerics  percent  country_Albania       ...         variety_Xynisteri  \\\n",
       "0         0        1                0       ...                         0   \n",
       "1         0        0                0       ...                         0   \n",
       "\n",
       "   variety_Yapincak  variety_Zelen  variety_Zibibbo  variety_Zierfandler  \\\n",
       "0                 0              0                0                    0   \n",
       "1                 0              0                0                    0   \n",
       "\n",
       "   variety_Zierfandler-Rotgipfler  variety_Zinfandel  variety_Zlahtina  \\\n",
       "0                               0                  0                 0   \n",
       "1                               0                  0                 0   \n",
       "\n",
       "   variety_Zweigelt  variety_Žilavka  \n",
       "0                 0                0  \n",
       "1                 0                0  \n",
       "\n",
       "[2 rows x 2398 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine description and designation into 1 column\n",
    "df['description'] = df['designation'].astype(str) + ' ' + df['winery'] + ' ' + df['description']\n",
    "\n",
    "#get rid of description so you can get dummies\n",
    "df_description = df[['description']]\n",
    "df.drop(['description','designation','winery'], axis=1, inplace=True)\n",
    "df = pd.get_dummies(df)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tfidf features\n",
    "#TF-IDF:  NLP technicque that is intended to reflect how important a word is to the description\n",
    "#in the collection of all descriptions.  Here, we take the top 1,000 top words\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    "stop_words= 'english',ngram_range=(1,1))\n",
    "vect = tfidf.fit_transform(df_description['description'])\n",
    "\n",
    "columns = tfidf.get_feature_names()\n",
    "\n",
    "a = vect.toarray()\n",
    "\n",
    "df2 = pd.DataFrame(np.reshape(a, (len(a),vect.shape[1])),columns=columns)\n",
    "df = pd.concat([df, df2],axis=1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>is_exceptional</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>numerics</th>\n",
       "      <th>percent</th>\n",
       "      <th>country_Albania</th>\n",
       "      <th>...</th>\n",
       "      <th>years</th>\n",
       "      <th>yeasty</th>\n",
       "      <th>yellow</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>youthful</th>\n",
       "      <th>zest</th>\n",
       "      <th>zesty</th>\n",
       "      <th>zin</th>\n",
       "      <th>zinfandel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96</td>\n",
       "      <td>235.0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>355</td>\n",
       "      <td>4.933333</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>318</td>\n",
       "      <td>5.254902</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 3398 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   points  price  is_exceptional  word_count  char_count  avg_word  stopwords  \\\n",
       "0      96  235.0               1          60         355  4.933333         23   \n",
       "1      96  110.0               1          51         318  5.254902         18   \n",
       "\n",
       "   numerics  percent  country_Albania    ...         years  yeasty  yellow  \\\n",
       "0         0        1                0    ...      0.305321     0.0     0.0   \n",
       "1         0        0                0    ...      0.000000     0.0     0.0   \n",
       "\n",
       "   youll  young  youthful  zest  zesty  zin  zinfandel  \n",
       "0    0.0    0.0       0.0   0.0    0.0  0.0        0.0  \n",
       "1    0.0    0.0       0.0   0.0    0.0  0.0        0.0  \n",
       "\n",
       "[2 rows x 3398 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Definine outcome and predictors.\n",
    "y = df['is_exceptional']\n",
    "X = df.loc[:, ~df.columns.isin(['is_exceptional','points'])]\n",
    "\n",
    "#Definine outcome and predictors with price removed.\n",
    "y_np = df['is_exceptional']\n",
    "X_np = df.loc[:, ~df.columns.isin(['is_exceptional','points','price'])]\n",
    "\n",
    "#Definine outcome and predictors with price removed.\n",
    "y_points = df['points']\n",
    "X_points = df.loc[:, ~df.columns.isin(['is_exceptional','points'])]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create/Load the test and train set\n",
    "if not os.path.exists('x_train.pkl') & os.path.exists('y_train.pkl') & os.path.exists('x_test.pkl') & os.path.exists('y_test.pkl'):\n",
    "    # Create training and test sets.\n",
    "    X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.3)\n",
    "    \n",
    "    pickle.dump(X_train, open('x_train.pkl', 'wb'))\n",
    "    pickle.dump(X_test, open('x_test.pkl', 'wb'))\n",
    "    pickle.dump(y_train, open('y_train.pkl', 'wb'))\n",
    "    pickle.dump(y_test, open('y_test.pkl', 'wb'))\n",
    "else:\n",
    "    X_train = pickle.load(open('x_train.pkl', 'rb'))\n",
    "    X_test = pickle.load(open('x_test.pkl', 'rb'))\n",
    "    y_train = pickle.load(open('y_train.pkl', 'rb'))\n",
    "    y_test = pickle.load(open('y_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_name, rf_filename, bool_feature_selection, bool_pca, num_components, clf):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not os.path.exists(rf_filename):\n",
    "        # Initialize and fit the model.\n",
    "        scaler =  StandardScaler()\n",
    "\n",
    "        if bool_feature_selection == True & bool_pca == True:\n",
    "            #Set PCA\n",
    "            sklearn_pca = PCA(n_components=num_components)\n",
    "            pipeline = Pipeline([\n",
    "            ('scale', scaler),\n",
    "            ('feature_selection', SelectFromModel(LinearSVC(loss='l2', penalty='l1', dual=False))),\n",
    "            ('dimensions', sklearn_pca),\n",
    "            ('classifier', clf)\n",
    "            ])\n",
    "        elif bool_feature_selection == True & bool_pca == False:\n",
    "            #Set PCA\n",
    "            sklearn_pca = PCA(n_components=num_components)\n",
    "            pipeline = Pipeline([\n",
    "            ('scale', scaler),\n",
    "            ('feature_selection', SelectFromModel(LinearSVC(loss='l2', penalty='l1', dual=False))),\n",
    "            ('classifier', clf)\n",
    "            ])\n",
    "        elif bool_feature_selection == False & bool_pca == True:\n",
    "            #Set PCA\n",
    "            sklearn_pca = PCA(n_components=num_components)\n",
    "            pipeline = Pipeline([\n",
    "            ('scale', scaler),\n",
    "            ('dimensions', sklearn_pca),\n",
    "            ('classifier', clf)\n",
    "            ])       \n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "            ('scale', scaler),\n",
    "            ('classifier', clf)\n",
    "            ])\n",
    "                \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        pickle.dump(pipeline, open(rf_filename, 'wb'))\n",
    "    else:\n",
    "        pipeline = pickle.load(open(rf_filename, 'rb'))\n",
    "\n",
    "\n",
    "    print(model_name)\n",
    "    print('Training set:')\n",
    "    print(classification_report(y_train, pipeline.predict(X_train)))\n",
    "    print('Test set:')\n",
    "    print(classification_report(y_test, pipeline.predict(X_test)))\n",
    "    print('Accuracy:  {}'.format(accuracy_score(y_test, pipeline.predict(X_test))))\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\classes.py:219: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Neural Net\n",
    "#rf_filename = r'neural_net.pkl'\n",
    "#pipeline = run_model('Neural Network', rf_filename, False, False, 0, MLPClassifier(hidden_layer_sizes=(10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish and fit the model, with a single, 1000 perceptron layer.\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lkonig\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:614: RuntimeWarning: invalid value encountered in greater\n",
      "  y = np.array(y > threshold, dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.98      0.97     71916\n",
      "          1       0.96      0.93      0.94     33735\n",
      "\n",
      "avg / total       0.96      0.96      0.96    105651\n",
      "\n",
      "Test set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.94      0.92     30804\n",
      "          1       0.85      0.81      0.83     14475\n",
      "\n",
      "avg / total       0.89      0.89      0.89     45279\n",
      "\n",
      "Accuracy:  0.8939905916650103\n"
     ]
    }
   ],
   "source": [
    "print('Training set:')\n",
    "print(classification_report(y_train, mlp.predict(X_train)))\n",
    "print('Test set:')\n",
    "print(classification_report(y_test, mlp.predict(X_test)))\n",
    "print('Accuracy:  {}'.format(accuracy_score(y_test, mlp.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     71916\n",
      "          1       1.00      0.98      0.99     33735\n",
      "\n",
      "avg / total       0.99      0.99      0.99    105651\n",
      "\n",
      "Test set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.95      0.94     30804\n",
      "          1       0.89      0.83      0.86     14475\n",
      "\n",
      "avg / total       0.91      0.91      0.91     45279\n",
      "\n",
      "Accuracy:  0.9130501998719053\n"
     ]
    }
   ],
   "source": [
    "# Establish and fit the model, with a single, 1000 perceptron layer.\n",
    "mlp = MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)\n",
    "mlp.fit(X_train, y_train)\n",
    "print('Training set:')\n",
    "print(classification_report(y_train, mlp.predict(X_train)))\n",
    "print('Test set:')\n",
    "print(classification_report(y_test, mlp.predict(X_test)))\n",
    "print('Accuracy:  {}'.format(accuracy_score(y_test, mlp.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97     71916\n",
      "          1       1.00      0.89      0.94     33735\n",
      "\n",
      "avg / total       0.96      0.96      0.96    105651\n",
      "\n",
      "Test set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.96      0.92     30804\n",
      "          1       0.90      0.74      0.81     14475\n",
      "\n",
      "avg / total       0.89      0.89      0.89     45279\n",
      "\n",
      "Accuracy:  0.8915391240972637\n"
     ]
    }
   ],
   "source": [
    "# Establish and fit the model, with a single, 1000 perceptron layer.\n",
    "mlp = MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(100,4,), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)\n",
    "mlp.fit(X_train, y_train)\n",
    "print('Training set:')\n",
    "print(classification_report(y_train, mlp.predict(X_train)))\n",
    "print('Test set:')\n",
    "print(classification_report(y_test, mlp.predict(X_test)))\n",
    "print('Accuracy:  {}'.format(accuracy_score(y_test, mlp.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     71916\n",
      "          1       1.00      0.99      0.99     33735\n",
      "\n",
      "avg / total       1.00      1.00      1.00    105651\n",
      "\n",
      "Test set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.96      0.94     30804\n",
      "          1       0.90      0.84      0.87     14475\n",
      "\n",
      "avg / total       0.92      0.92      0.92     45279\n",
      "\n",
      "Accuracy:  0.917666026193158\n"
     ]
    }
   ],
   "source": [
    "# Establish and fit the model, with a single, 1000 perceptron layer.\n",
    "mlp = MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(200,), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)\n",
    "mlp.fit(X_train, y_train)\n",
    "print('Training set:')\n",
    "print(classification_report(y_train, mlp.predict(X_train)))\n",
    "print('Test set:')\n",
    "print(classification_report(y_test, mlp.predict(X_test)))\n",
    "print('Accuracy:  {}'.format(accuracy_score(y_test, mlp.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     71916\n",
      "          1       0.99      0.99      0.99     33735\n",
      "\n",
      "avg / total       0.99      0.99      0.99    105651\n",
      "\n",
      "Test set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94     30804\n",
      "          1       0.87      0.87      0.87     14475\n",
      "\n",
      "avg / total       0.92      0.92      0.92     45279\n",
      "\n",
      "Accuracy:  0.9166942732834206\n"
     ]
    }
   ],
   "source": [
    "# Establish and fit the model, with a single, 1000 perceptron layer.\n",
    "mlp = MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(300,), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)\n",
    "mlp.fit(X_train, y_train)\n",
    "print('Training set:')\n",
    "print(classification_report(y_train, mlp.predict(X_train)))\n",
    "print('Test set:')\n",
    "print(classification_report(y_test, mlp.predict(X_test)))\n",
    "print('Accuracy:  {}'.format(accuracy_score(y_test, mlp.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      0.99     71916\n",
      "          1       0.97      0.99      0.98     33735\n",
      "\n",
      "avg / total       0.99      0.99      0.99    105651\n",
      "\n",
      "Test set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.92      0.93     30804\n",
      "          1       0.84      0.88      0.86     14475\n",
      "\n",
      "avg / total       0.91      0.91      0.91     45279\n",
      "\n",
      "Accuracy:  0.9078380706287683\n"
     ]
    }
   ],
   "source": [
    "# Establish and fit the model, with a single, 1000 perceptron layer.\n",
    "mlp = MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(200,20), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)\n",
    "mlp.fit(X_train, y_train)\n",
    "print('Training set:')\n",
    "print(classification_report(y_train, mlp.predict(X_train)))\n",
    "print('Test set:')\n",
    "print(classification_report(y_test, mlp.predict(X_test)))\n",
    "print('Accuracy:  {}'.format(accuracy_score(y_test, mlp.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-9509cfda7591>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m        \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m        verbose=False, warm_start=False)\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training set:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    971\u001b[0m         \"\"\"\n\u001b[0;32m    972\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[1;32m--> 973\u001b[1;33m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[0;32m    974\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    364\u001b[0m         activations.extend(np.empty((batch_size, n_fan_out))\n\u001b[0;32m    365\u001b[0m                            for n_fan_out in layer_units[1:])\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mdeltas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_layer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma_layer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         coef_grads = [np.empty((n_fan_in_, n_fan_out_)) for n_fan_in_,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    364\u001b[0m         activations.extend(np.empty((batch_size, n_fan_out))\n\u001b[0;32m    365\u001b[0m                            for n_fan_out in layer_units[1:])\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mdeltas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_layer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma_layer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         coef_grads = [np.empty((n_fan_in_, n_fan_out_)) for n_fan_in_,\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Establish and fit the model, with a single, 1000 perceptron layer.\n",
    "mlp = MLPClassifier(activation='logistic', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(200,), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)\n",
    "mlp.fit(X_train, y_train)\n",
    "print('Training set:')\n",
    "print(classification_report(y_train, mlp.predict(X_train)))\n",
    "print('Test set:')\n",
    "print(classification_report(y_test, mlp.predict(X_test)))\n",
    "print('Accuracy:  {}'.format(accuracy_score(y_test, mlp.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
